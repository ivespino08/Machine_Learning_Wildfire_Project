{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83330e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "#getting the dataset\n",
    "file_path = \"./Wildfire_Dataset.csv\"\n",
    "\n",
    "df = kagglehub.dataset_load(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"firecastrl/us-wildfire-dataset\",\n",
    "    file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b379ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m FEATURES = [\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mrmax\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mrmin\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msph\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msrad\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtmmn\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtmmx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mvs\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfm100\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfm1000\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33merc\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33metr\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mpet\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mvpd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m ]\n\u001b[32m     20\u001b[39m USE_COLS = [\u001b[33m\"\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mWildfire\u001b[39m\u001b[33m\"\u001b[39m] + FEATURES\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSE_COLS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m df = df.dropna()\n\u001b[32m     24\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/codecs.py:334\u001b[39m, in \u001b[36mBufferedIncrementalDecoder.getstate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    331\u001b[39m     IncrementalDecoder.reset(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    332\u001b[39m     \u001b[38;5;28mself\u001b[39m.buffer = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# additional state info is always 0\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.buffer, \u001b[32m0\u001b[39m)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# ignore additional state info\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#creating a sequence level sample (no more rows)\n",
    "INPUT_FILE = \"Wildfire_Dataset.csv\"\n",
    "OUTPUT_FILE = \"wildfire_sequence_sample.csv\"\n",
    "\n",
    "SEQ_LEN = 75\n",
    "FIRE_RATIO_THRESHOLD = 0.20\n",
    "#can be adjusted\n",
    "N_SEQUENCES_SAMPLE = 20000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    \"pr\",\"rmax\",\"rmin\",\"sph\",\"srad\",\n",
    "    \"tmmn\",\"tmmx\",\"vs\",\"bi\",\n",
    "    \"fm100\",\"fm1000\",\"erc\",\"etr\",\"pet\",\"vpd\"\n",
    "]\n",
    "\n",
    "USE_COLS = [\"latitude\",\"longitude\",\"datetime\",\"Wildfire\"] + FEATURES\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE, usecols=USE_COLS)\n",
    "df = df.dropna()\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "#75 day sequence per location\n",
    "seq_rows = []\n",
    "seq_labels = []\n",
    "seq_id = 0\n",
    "\n",
    "for (lat, lon), g in df.groupby([\"latitude\", \"longitude\"]):\n",
    "    g = g.sort_values(\"datetime\")\n",
    "\n",
    "    n_full = len(g) // SEQ_LEN\n",
    "    if n_full == 0:\n",
    "        continue\n",
    "\n",
    "    g = g.iloc[:n_full * SEQ_LEN]\n",
    "\n",
    "    for i in range(n_full):\n",
    "        block = g.iloc[i*SEQ_LEN:(i+1)*SEQ_LEN].copy()\n",
    "        block[\"SeqID\"] = seq_id\n",
    "\n",
    "        fire_ratio = (block[\"Wildfire\"] == \"Yes\").mean()\n",
    "        label = 1 if fire_ratio >= FIRE_RATIO_THRESHOLD else 0\n",
    "\n",
    "        block[\"SeqLabel\"] = label\n",
    "\n",
    "        seq_rows.append(block)\n",
    "        seq_labels.append((seq_id, label))\n",
    "        seq_id += 1\n",
    "\n",
    "seq_df = pd.concat(seq_rows, ignore_index=True)\n",
    "seq_label_df = pd.DataFrame(seq_labels, columns=[\"SeqID\", \"Label\"])\n",
    "\n",
    "print(\"Total sequences built:\", len(seq_label_df))\n",
    "print(\"Label distribution:\\n\", seq_label_df[\"Label\"].value_counts(normalize=True))\n",
    "\n",
    "#take sample\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "available_seq_ids = seq_label_df[\"SeqID\"].values\n",
    "n_sample = min(N_SEQUENCES_SAMPLE, len(available_seq_ids))\n",
    "\n",
    "sampled_seq_ids = rng.choice(\n",
    "    available_seq_ids,\n",
    "    size=n_sample,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "sample_df = seq_df[seq_df[\"SeqID\"].isin(sampled_seq_ids)].copy()\n",
    "sample_df = sample_df.sort_values([\"SeqID\", \"datetime\"])\n",
    "sample_df = sample_df.drop(columns=[\"SeqID\", \"SeqLabel\"])\n",
    "\n",
    "sample_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"Saved sample to:\", OUTPUT_FILE)\n",
    "print(\"Sample rows:\", len(sample_df))\n",
    "print(\"Sample sequences:\", len(sample_df) // SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ca78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "#used sampled file to speed up train and test process\n",
    "SAMPLE_FILE = \"wildfire_sequence_sample.csv\"\n",
    "SEQ_LEN = 75\n",
    "THRESHOLD = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    \"pr\",\"rmax\",\"rmin\",\"sph\",\"srad\",\n",
    "    \"tmmn\",\"tmmx\",\"vs\",\"bi\",\n",
    "    \"fm100\",\"fm1000\",\"erc\",\"etr\",\"pet\",\"vpd\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8aeae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences built: 20000\n",
      "Summary feature matrix shape: (20000, 90)\n",
      "Label distribution:\n",
      " 0    14878\n",
      "1     5122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(SAMPLE_FILE).dropna().copy()\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "seq_feature_rows = []\n",
    "seq_labels = []\n",
    "\n",
    "def summarize_block(block: pd.DataFrame) -> np.ndarray:\n",
    "\n",
    "    \"\"\"Convert a 75x15 block into a compact feature vector.\n",
    "    Stats per feature: mean, std, min, max, last value, slope (trend).\n",
    "    Output size = 15 * 6 = 90 features.\"\"\"\n",
    "\n",
    "    X = block[FEATURES].to_numpy()\n",
    "\n",
    "    mean = X.mean(axis=0)\n",
    "    std  = X.std(axis=0)\n",
    "    mn   = X.min(axis=0)\n",
    "    mx   = X.max(axis=0)\n",
    "    last = X[-1, :]\n",
    "\n",
    "    t = np.arange(X.shape[0])\n",
    "    var_t = t.var()\n",
    "    slope = ((t - t.mean())[:, None] * (X - X.mean(axis=0))).mean(axis=0) / var_t\n",
    "\n",
    "    return np.concatenate([mean, std, mn, mx, last, slope])\n",
    "\n",
    "seq_id = 0\n",
    "for (lat, lon), g in df.groupby([\"latitude\", \"longitude\"]):\n",
    "    g = g.sort_values(\"datetime\")\n",
    "    n_full = len(g) // SEQ_LEN\n",
    "    if n_full == 0:\n",
    "        continue\n",
    "\n",
    "    g = g.iloc[:n_full * SEQ_LEN]\n",
    "\n",
    "    for i in range(n_full):\n",
    "        block = g.iloc[i*SEQ_LEN:(i+1)*SEQ_LEN]\n",
    "\n",
    "        fire_ratio = (block[\"Wildfire\"] == \"Yes\").mean()\n",
    "        label = 1 if fire_ratio >= THRESHOLD else 0\n",
    "\n",
    "        seq_feature_rows.append(summarize_block(block))\n",
    "        seq_labels.append(label)\n",
    "        seq_id += 1\n",
    "\n",
    "X = np.vstack(seq_feature_rows)\n",
    "y = np.array(seq_labels)\n",
    "\n",
    "print(\"Sequences built:\", len(y))\n",
    "print(\"Summary feature matrix shape:\", X.shape)\n",
    "print(\"Label distribution:\\n\", pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a27000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (16000, 90) Test: (4000, 90)\n",
      "train labels:\n",
      " 0    11902\n",
      "1     4098\n",
      "Name: count, dtype: int64\n",
      "test labels:\n",
      " 0    2976\n",
      "1    1024\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print(\"train:\", X_train_s.shape, \"Test:\", X_test_s.shape)\n",
    "print(\"train labels:\\n\", pd.Series(y_train).value_counts())\n",
    "print(\"test labels:\\n\", pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ad02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.811     0.605     0.693      2976\n",
      "           1      0.339     0.589     0.430      1024\n",
      "\n",
      "    accuracy                          0.601      4000\n",
      "   macro avg      0.575     0.597     0.562      4000\n",
      "weighted avg      0.690     0.601     0.626      4000\n",
      "\n",
      "---confusion matrix---\n",
      "[[1801 1175]\n",
      " [ 421  603]]\n",
      "\n",
      "Accuracy: 0.601\n",
      "Balanced accuracy: 0.597\n",
      "ROC-AUC: 0.653\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0, #maybe use bigger C\n",
    "    gamma=\"scale\",\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "svm.fit(X_train_s, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test_s)\n",
    "\n",
    "print(\"---classification report---\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"---confusion matrix---\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "scores = svm.decision_function(X_test_s)\n",
    "auc = roc_auc_score(y_test, scores)\n",
    "\n",
    "print(\"\\naccuracy:\", round(acc, 3))\n",
    "print(\"balanced accuracy:\", round(bal_acc, 3))\n",
    "print(\"ROC-AUC:\", round(auc, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Environment for OKC)",
   "language": "python",
   "name": "my_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
